{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f4b858c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded: ./final_unclean.csv shape: (1175, 14)\n",
      "['Country Name', 'ISO3', 'Year', 'HDI', 'Suicide_rate', 'GDP_per_capita', 'log_GDP_per_capita', 'income_group_auto', 'continent', 'Low_data_quality_flag', 'HDI_sq', 'Suicide_rate_lag1', 'HDI_lag1', 'Low_data_quality_flag_panel']\n",
      "Numeric columns recognized: ['HDI', 'Suicide_rate', 'GDP_per_capita', 'log_GDP_per_capita']\n",
      "Imputing: HDI\n",
      "Imputing: Suicide_rate\n",
      "Imputing: GDP_per_capita\n",
      "Imputing: log_GDP_per_capita\n",
      "EDA summary saved to: final_output\\eda_summary.txt\n",
      "Saved plots to: final_output\\plots\n",
      "Modeling observations: 1175\n",
      "Pooled OLS formula: Suicide_rate ~ HDI_centered + HDI_centered_sq + log_GDP_per_capita + Y_2019 + Y_2020 + Y_2021 + Y_2022 + Y_2023 + cont_America + cont_Asia + cont_Europe + cont_Oceania\n",
      "Saved pooled OLS summary.\n",
      "PanelOLS not available or failed: No module named 'linearmodels'\n",
      "Fallback country-dummy model failed: invalid syntax (<unknown>, line 1)\n",
      "Saved final dataset to: final_output\\final_dataset.csv\n",
      "Pipeline complete. Outputs in: final_output\n",
      "Final dataset shape: (1175, 44)\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Full pipeline:\n",
    "- Load new_merged_from_time_series.csv\n",
    "- Clean & impute (no deletion): interpolation per-country -> forward/backfill -> region median -> global median\n",
    "- Add imputation flags (per variable)\n",
    "- Feature engineering (HDI_sq, HDI_centered, log GDP, lags, pct_change, 3-yr rolling)\n",
    "- EDA: summary, correlations, plots saved to ./final_output/\n",
    "- Modelling: pooled OLS, robust OLS, panel FE (linearmodels PanelOLS if installed)\n",
    "- Save final_dataset.csv and model summaries\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "from pathlib import Path\n",
    "import warnings\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import statsmodels.formula.api as smf\n",
    "import statsmodels.api as sm\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "sns.set(style=\"whitegrid\", font_scale=1.0)\n",
    "\n",
    "# ---------- CONFIG ----------\n",
    "INPUT_CSV = \"./final_unclean.csv\"   # change if path differs\n",
    "OUTDIR = Path(\"./final_output\")\n",
    "OUTDIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "FINAL_CSV = OUTDIR / \"final_dataset.csv\"\n",
    "EDA_SUMMARY = OUTDIR / \"eda_summary.txt\"\n",
    "PLOTS_DIR = OUTDIR / \"plots\"\n",
    "PLOTS_DIR.mkdir(exist_ok=True)\n",
    "\n",
    "# ---------- LOAD ----------\n",
    "df = pd.read_csv(INPUT_CSV)\n",
    "print(\"Loaded:\", INPUT_CSV, \"shape:\", df.shape)\n",
    "\n",
    "# Standardize columns we will work with\n",
    "# Ensure column names trimmed\n",
    "df.columns = [c.strip() for c in df.columns]\n",
    "\n",
    "# Show basic info\n",
    "print(df.columns.tolist())\n",
    "\n",
    "# ---------- PREPARATION ----------\n",
    "# Ensure Year numeric\n",
    "if 'Year' in df.columns:\n",
    "    df['Year'] = pd.to_numeric(df['Year'], errors='coerce')\n",
    "else:\n",
    "    raise ValueError(\"The input file must contain 'Year' column for panel structure.\")\n",
    "\n",
    "# Ensure ISO3 present (we expect ISO3)\n",
    "if 'ISO3' not in df.columns:\n",
    "    # optionally create from 'Country Name' via country_converter if needed\n",
    "    df['ISO3'] = np.nan\n",
    "\n",
    "# Convert common numeric columns\n",
    "num_cols = []\n",
    "for c in ['HDI', 'Suicide_rate', 'GDP_per_capita', 'log_GDP_per_capita']:\n",
    "    if c in df.columns:\n",
    "        num_cols.append(c)\n",
    "        df[c] = pd.to_numeric(df[c], errors='coerce')\n",
    "\n",
    "print(\"Numeric columns recognized:\", num_cols)\n",
    "\n",
    "# ---------- IMPUTATION STRATEGY ----------\n",
    "# We'll impute numeric variables as follows (per variable):\n",
    "# 1) Per-country linear interpolation across Year (sorted) for small gaps\n",
    "# 2) Per-country forward-fill then back-fill (to carry last known value)\n",
    "# 3) If still missing, fill with region (continent) median for that Year (if available)\n",
    "# 4) If still missing, fill with global median (or mean) â€” but we record imputation\n",
    "# For categorical (income_group_auto, Low_data_quality_flag, continent): fill missing with mode per region or 'Unknown'\n",
    "\n",
    "# We'll create imputation flag columns: <var>_imputed (bool) and <var>_imputed_method (text)\n",
    "\n",
    "df_out = df.copy()\n",
    "\n",
    "# Helper: create per-var flags and apply steps\n",
    "def impute_numeric_panel(df_panel, var, country_col='ISO3', year_col='Year', region_col='continent'):\n",
    "    \"\"\"\n",
    "    Returns df with var imputed. Adds columns:\n",
    "    var + \"_imputed\" (bool)\n",
    "    var + \"_imputed_method\" (str)\n",
    "    \"\"\"\n",
    "    dfp = df_panel.copy()\n",
    "    flag_col = f\"{var}_imputed\"\n",
    "    method_col = f\"{var}_imputed_method\"\n",
    "    dfp[flag_col] = False\n",
    "    dfp[method_col] = \"original\"\n",
    "\n",
    "    # Track where originally missing\n",
    "    orig_na = dfp[var].isna()\n",
    "\n",
    "    # --- Step 1: Per-country interpolation (fixed alignment) ---\n",
    "    dfp = dfp.sort_values([country_col, year_col])\n",
    "    interp = (\n",
    "        dfp.groupby(country_col, group_keys=True)[[year_col, var]]\n",
    "        .apply(lambda g: g.set_index(year_col)[var].interpolate(method='linear', limit_direction='both'))\n",
    "        .reset_index()  # reset within each group\n",
    "        .rename(columns={var: f\"{var}_interp\"})\n",
    "    )\n",
    "    # Now interp contains: [ISO3, Year, var_interp]\n",
    "    dfp = dfp.merge(interp, on=[country_col, year_col], how='left')\n",
    "    dfp[var] = dfp[f\"{var}_interp\"].combine_first(dfp[var])\n",
    "    dfp.drop(columns=[f\"{var}_interp\"], inplace=True)\n",
    "\n",
    "    interp_mask = orig_na & dfp[var].notna()\n",
    "    dfp.loc[interp_mask, flag_col] = True\n",
    "    dfp.loc[interp_mask, method_col] = \"interpolated\"\n",
    "\n",
    "    # --- Step 2: Forward/back fill per-country ---\n",
    "    before_na = dfp[var].isna()\n",
    "    dfp[var] = (\n",
    "        dfp.groupby(country_col, group_keys=False)[var]\n",
    "        .apply(lambda s: s.ffill().bfill())\n",
    "    )\n",
    "    ffill_mask = before_na & dfp[var].notna()\n",
    "    dfp.loc[ffill_mask, flag_col] = True\n",
    "    dfp.loc[ffill_mask, method_col] = \"ffill_bfill\"\n",
    "\n",
    "    # --- Step 3: Region-year median ---\n",
    "    if region_col in dfp.columns:\n",
    "        region_median = (\n",
    "            dfp.groupby([region_col, year_col])[var]\n",
    "            .transform('median')\n",
    "        )\n",
    "        before_na = dfp[var].isna()\n",
    "        dfp[var] = dfp[var].fillna(region_median)\n",
    "        region_mask = before_na & dfp[var].notna()\n",
    "        dfp.loc[region_mask, flag_col] = True\n",
    "        dfp.loc[region_mask, method_col] = \"region_year_median\"\n",
    "\n",
    "    # --- Step 4: Global median fallback ---\n",
    "    before_na = dfp[var].isna()\n",
    "    global_median = dfp[var].median(skipna=True)\n",
    "    dfp[var] = dfp[var].fillna(global_median)\n",
    "    global_mask = before_na & dfp[var].notna()\n",
    "    dfp.loc[global_mask, flag_col] = True\n",
    "    dfp.loc[global_mask, method_col] = \"global_median\"\n",
    "\n",
    "    # Mark remaining missing (if any)\n",
    "    still_na = dfp[var].isna()\n",
    "    if still_na.any():\n",
    "        dfp.loc[still_na, flag_col] = True\n",
    "        dfp.loc[still_na, method_col] = \"unfilled\"\n",
    "\n",
    "    return dfp[[country_col, year_col, var, flag_col, method_col]]\n",
    "\n",
    "\n",
    "\n",
    "# Apply for each numeric var we care about\n",
    "vars_to_impute = [v for v in ['HDI', 'Suicide_rate', 'GDP_per_capita', 'log_GDP_per_capita'] if v in df_out.columns]\n",
    "\n",
    "# We'll iteratively merge results of imputation back into df_out\n",
    "for var in vars_to_impute:\n",
    "    print(\"Imputing:\", var)\n",
    "    res = impute_numeric_panel(df_out, var, country_col='ISO3', year_col='Year', region_col='continent')\n",
    "    df_out = df_out.drop(columns=[var], errors='ignore').merge(\n",
    "        res, on=['ISO3', 'Year'], how='left'\n",
    "    )\n",
    "\n",
    "# ---------- CATEGORICAL / GROUP FILL ----------\n",
    "# For income_group_auto, continent, Low_data_quality_flag: fill missing with country-mode, then region-mode, then 'Unknown'\n",
    "def fill_categorical(dfp, col, country_col='ISO3', region_col='continent'):\n",
    "    new_flag = f\"{col}_filled\"\n",
    "    dfp[new_flag] = False\n",
    "    # try country-level mode\n",
    "    country_mode = dfp.groupby(country_col)[col].agg(lambda s: s.dropna().mode().iloc[0] if s.dropna().shape[0]>0 else np.nan)\n",
    "    dfp[col] = dfp.apply(lambda r: country_mode.get(r[country_col]) if pd.isna(r.get(col)) and r[country_col] in country_mode.index else r.get(col), axis=1)\n",
    "    filled_country = dfp[col].notna()\n",
    "    dfp.loc[filled_country, new_flag] = True\n",
    "\n",
    "    # region-level\n",
    "    if region_col in dfp.columns:\n",
    "        region_mode = dfp.groupby(region_col)[col].agg(lambda s: s.dropna().mode().iloc[0] if s.dropna().shape[0]>0 else np.nan)\n",
    "        dfp[col] = dfp.apply(lambda r: region_mode.get(r[region_col]) if pd.isna(r.get(col)) and r.get(region_col) in region_mode.index else r.get(col), axis=1)\n",
    "        filled_region = dfp[col].notna()\n",
    "        dfp.loc[filled_region, new_flag] = True\n",
    "\n",
    "    # global fallback\n",
    "    global_mode = dfp[col].mode().iloc[0] if dfp[col].dropna().shape[0]>0 else \"Unknown\"\n",
    "    before = dfp[col].isna()\n",
    "    dfp[col] = dfp[col].fillna(global_mode)\n",
    "    after = dfp[col].notna()\n",
    "    filled_global = before & after\n",
    "    dfp.loc[filled_global, new_flag] = True\n",
    "\n",
    "    return dfp\n",
    "\n",
    "for cat in ['income_group_auto','continent','Low_data_quality_flag']:\n",
    "    if cat in df_out.columns:\n",
    "        df_out = fill_categorical(df_out, cat, country_col='ISO3', region_col='continent')\n",
    "\n",
    "# ---------- CREATE FINAL IMPUTATION SUMMARY COLUMNS ----------\n",
    "# Count for each row how many numeric vars were imputed\n",
    "imputed_flag_cols = [c for c in df_out.columns if c.endswith('_imputed')]\n",
    "df_out['num_imputed_numeric'] = df_out[imputed_flag_cols].sum(axis=1)\n",
    "\n",
    "# Also store methods in compact form\n",
    "method_cols = [c for c in df_out.columns if c.endswith('_imputed_method')]\n",
    "df_out['imputation_methods'] = df_out[method_cols].apply(lambda row: \";\".join(sorted(set([str(x) for x in row if pd.notna(x) and x!='original']))), axis=1)\n",
    "\n",
    "# ---------- FEATURE ENGINEERING ----------\n",
    "# HDI squared (if not present)\n",
    "if 'HDI_sq' not in df_out.columns and 'HDI' in df_out.columns:\n",
    "    df_out['HDI_sq'] = df_out['HDI']**2\n",
    "\n",
    "# HDI centered (to reduce collinearity with HDI_sq)\n",
    "if 'HDI' in df_out.columns:\n",
    "    df_out['HDI_centered'] = df_out['HDI'] - df_out['HDI'].mean()\n",
    "\n",
    "# log GDP: ensure presence and create if missing\n",
    "if 'log_GDP_per_capita' not in df_out.columns and 'GDP_per_capita' in df_out.columns:\n",
    "    df_out['log_GDP_per_capita'] = np.log(df_out['GDP_per_capita'].where(df_out['GDP_per_capita']>0, np.nan))\n",
    "    # mark as imputed by earlier pipeline\n",
    "\n",
    "# LAG features: lag 1 of HDI and Suicide_rate per country\n",
    "df_out = df_out.sort_values(['ISO3','Year'])\n",
    "for v in ['HDI','Suicide_rate','GDP_per_capita','log_GDP_per_capita']:\n",
    "    if v in df_out.columns:\n",
    "        lag_col = f\"{v}_lag1\"\n",
    "        df_out[lag_col] = df_out.groupby('ISO3')[v].shift(1)\n",
    "        # create indicator if lag was imputed (based on existence of original var imputation)\n",
    "        imp_col = f\"{v}_lag1_imputed\"\n",
    "        df_out[imp_col] = df_out[lag_col].isna()\n",
    "\n",
    "# Percent change (year over year) for HDI and Suicide_rate\n",
    "for v in ['HDI','Suicide_rate','GDP_per_capita']:\n",
    "    if v in df_out.columns:\n",
    "        df_out[f\"{v}_pct_change\"] = df_out.groupby('ISO3')[v].pct_change()\n",
    "\n",
    "# 3-year rolling mean (centered) where available\n",
    "for v in ['HDI','Suicide_rate']:\n",
    "    if v in df_out.columns:\n",
    "        df_out[f\"{v}_roll3\"] = df_out.groupby('ISO3')[v].rolling(window=3, min_periods=1).mean().reset_index(level=0, drop=True)\n",
    "\n",
    "# Z-score within-year (for comparability across countries)\n",
    "for v in ['HDI','Suicide_rate','GDP_per_capita','log_GDP_per_capita']:\n",
    "    if v in df_out.columns:\n",
    "        df_out[f\"{v}_z_year\"] = df_out.groupby('Year')[v].transform(lambda x: (x - x.mean())/x.std(ddof=0))\n",
    "\n",
    "# ---------- ETHICAL / TRANSPARENCY COLUMNS ----------\n",
    "# Add column that summarizes data quality: number of imputation methods + Low_data_quality_flag_panel if exists\n",
    "if 'num_imputed_numeric' not in df_out.columns:\n",
    "    df_out['num_imputed_numeric'] = np.nan\n",
    "df_out['data_quality_summary'] = df_out.apply(\n",
    "    lambda r: f\"{r.get('Low_data_quality_flag_panel', r.get('Low_data_quality_flag','Unknown'))} | imputed_vars={int(r['num_imputed_numeric'])} | methods={r.get('imputation_methods','')}\",\n",
    "    axis=1\n",
    ")\n",
    "\n",
    "# ---------- EDA: summary & plots ----------\n",
    "# Save simple EDA summary text\n",
    "with open(EDA_SUMMARY, \"w\", encoding=\"utf-8\") as f:\n",
    "    f.write(\"EDA Summary - final_dataset\\n\")\n",
    "    f.write(f\"Input file: {INPUT_CSV}\\n\")\n",
    "    f.write(f\"Rows: {len(df_out)}, Unique countries (ISO3): {df_out['ISO3'].nunique()}\\n\\n\")\n",
    "    f.write(\"Numeric descriptive stats (selected vars):\\n\")\n",
    "    desc_cols = [c for c in ['HDI','Suicide_rate','GDP_per_capita','log_GDP_per_capita'] if c in df_out.columns]\n",
    "    f.write(df_out[desc_cols].describe().to_string())\n",
    "    f.write(\"\\n\\nImputation overview (per variable):\\n\")\n",
    "    for var in vars_to_impute:\n",
    "        flag = f\"{var}_imputed\"\n",
    "        if flag in df_out.columns:\n",
    "            n_imputed = df_out[flag].sum()\n",
    "            f.write(f\"{var}: imputed count = {int(n_imputed)} / {len(df_out)}\\n\")\n",
    "    f.write(\"\\nNotes:\\n- Imputation methods are encoded in *_imputed_method columns.\\n- data_quality_summary column provides quick quality flags.\\n\")\n",
    "print(\"EDA summary saved to:\", EDA_SUMMARY)\n",
    "\n",
    "# Basic plots: distribution & trend plots\n",
    "# 1) HDI distribution\n",
    "if 'HDI' in df_out.columns:\n",
    "    plt.figure(figsize=(8,4))\n",
    "    sns.histplot(df_out['HDI'].dropna(), bins=30, kde=True)\n",
    "    plt.title(\"HDI distribution (all years)\")\n",
    "    plt.savefig(PLOTS_DIR / \"hdi_distribution.png\", dpi=150)\n",
    "    plt.close()\n",
    "\n",
    "# 2) Suicide_rate distribution\n",
    "if 'Suicide_rate' in df_out.columns:\n",
    "    plt.figure(figsize=(8,4))\n",
    "    sns.histplot(df_out['Suicide_rate'].dropna(), bins=30, kde=True)\n",
    "    plt.title(\"Suicide rate distribution (all years)\")\n",
    "    plt.savefig(PLOTS_DIR / \"suicide_distribution.png\", dpi=150)\n",
    "    plt.close()\n",
    "\n",
    "# 3) HDI trend sample by continent\n",
    "if 'Year' in df_out.columns and 'continent' in df_out.columns and 'HDI' in df_out.columns:\n",
    "    plt.figure(figsize=(10,6))\n",
    "    sns.lineplot(data=df_out, x='Year', y='HDI', hue='continent', estimator='median')\n",
    "    plt.title(\"Median HDI trend by continent\")\n",
    "    plt.savefig(PLOTS_DIR / \"hdi_trend_by_continent.png\", dpi=150)\n",
    "    plt.close()\n",
    "\n",
    "# 4) Suicide trend by continent\n",
    "if 'Year' in df_out.columns and 'continent' in df_out.columns and 'Suicide_rate' in df_out.columns:\n",
    "    plt.figure(figsize=(10,6))\n",
    "    sns.lineplot(data=df_out, x='Year', y='Suicide_rate', hue='continent', estimator='median')\n",
    "    plt.title(\"Median Suicide rate trend by continent\")\n",
    "    plt.savefig(PLOTS_DIR / \"suicide_trend_by_continent.png\", dpi=150)\n",
    "    plt.close()\n",
    "\n",
    "# 5) Scatter HDI vs Suicide colored by Year (sample)\n",
    "if 'HDI' in df_out.columns and 'Suicide_rate' in df_out.columns:\n",
    "    plt.figure(figsize=(8,6))\n",
    "    sns.scatterplot(data=df_out, x='HDI', y='Suicide_rate', hue='Year', palette='viridis', alpha=0.8)\n",
    "    plt.title(\"HDI vs Suicide (panel)\")\n",
    "    plt.savefig(PLOTS_DIR / \"scatter_hdi_suicide_panel.png\", dpi=150)\n",
    "    plt.close()\n",
    "\n",
    "print(\"Saved plots to:\", PLOTS_DIR)\n",
    "\n",
    "# ---------- MODELLING ----------\n",
    "# We'll run:\n",
    "# 1) Pooled OLS: Suicide_rate ~ HDI + HDI_centered^2 + log_GDP_per_capita + Year dummies + continent dummies\n",
    "# 2) Country Fixed Effects using linearmodels.PanelOLS if available (preferred)\n",
    "# 3) Robust OLS with HC1 (statsmodels)\n",
    "\n",
    "# Prepare modeling dataframe: keep rows with Suicide_rate & HDI present after imputation\n",
    "mod_df = df_out.copy()\n",
    "# Ensure numeric cast\n",
    "for v in ['Suicide_rate','HDI','HDI_sq','HDI_centered','log_GDP_per_capita']:\n",
    "    if v in mod_df.columns:\n",
    "        mod_df[v] = pd.to_numeric(mod_df[v], errors='coerce')\n",
    "\n",
    "# Fill small missing log_GDP with global median if still missing (we already did but ensure)\n",
    "if 'log_GDP_per_capita' in mod_df.columns:\n",
    "    mod_df['log_GDP_per_capita'] = mod_df['log_GDP_per_capita'].fillna(mod_df['log_GDP_per_capita'].median())\n",
    "\n",
    "# Model formula\n",
    "# Use centered HDI and its square to reduce collinearity\n",
    "if 'HDI_centered' in mod_df.columns:\n",
    "    mod_df['HDI_centered_sq'] = mod_df['HDI_centered']**2\n",
    "    predictors = ['HDI_centered', 'HDI_centered_sq']\n",
    "else:\n",
    "    predictors = ['HDI', 'HDI_sq']\n",
    "\n",
    "if 'log_GDP_per_capita' in mod_df.columns:\n",
    "    predictors.append('log_GDP_per_capita')\n",
    "\n",
    "# Year dummies and continent dummies\n",
    "if 'Year' in mod_df.columns:\n",
    "    year_dummies = pd.get_dummies(mod_df['Year'].astype(int).astype(str), prefix='Y', drop_first=True)\n",
    "    mod_df = pd.concat([mod_df, year_dummies], axis=1)\n",
    "    predictors += list(year_dummies.columns)\n",
    "\n",
    "if 'continent' in mod_df.columns:\n",
    "    cont_dummies = pd.get_dummies(mod_df['continent'], prefix='cont', drop_first=True)\n",
    "    mod_df = pd.concat([mod_df, cont_dummies], axis=1)\n",
    "    predictors += list(cont_dummies.columns)\n",
    "\n",
    "# Keep only rows where dependent & main regressors exist\n",
    "reg_needed = ['Suicide_rate'] + [p for p in predictors if p in mod_df.columns]\n",
    "mod_df_reg = mod_df.dropna(subset=reg_needed).copy()\n",
    "print(\"Modeling observations:\", len(mod_df_reg))\n",
    "\n",
    "# Pooled OLS with robust SE\n",
    "formula = \"Suicide_rate ~ \" + \" + \".join([p for p in predictors if p in mod_df_reg.columns])\n",
    "print(\"Pooled OLS formula:\", formula)\n",
    "try:\n",
    "    pooled_res = smf.ols(formula=formula, data=mod_df_reg).fit(cov_type='HC1')\n",
    "    with open(OUTDIR / \"model_pooled_summary.txt\", \"w\", encoding=\"utf-8\") as f:\n",
    "        f.write(pooled_res.summary().as_text())\n",
    "    print(\"Saved pooled OLS summary.\")\n",
    "except Exception as e:\n",
    "    print(\"Pooled model failed:\", e)\n",
    "    pooled_res = None\n",
    "\n",
    "# Try Panel Fixed Effects with linearmodels.PanelOLS if installed\n",
    "try:\n",
    "    from linearmodels.panel import PanelOLS\n",
    "    # prepare multiindex\n",
    "    panel_df = mod_df.set_index(['ISO3','Year'])\n",
    "    # dependent and exog\n",
    "    exog = panel_df[[p for p in predictors if p in panel_df.columns]]\n",
    "    exog = sm.add_constant(exog)\n",
    "    dep = panel_df['Suicide_rate']\n",
    "    # PanelOLS with entity effects and time effects (if many years)\n",
    "    panel_model = PanelOLS(dep, exog, entity_effects=True, time_effects=True)\n",
    "    panel_res = panel_model.fit(cov_type='robust')\n",
    "    with open(OUTDIR / \"panel_fixed_effects_summary.txt\", \"w\", encoding=\"utf-8\") as f:\n",
    "        f.write(panel_res.summary.as_text())\n",
    "    print(\"Saved panel fixed effects summary.\")\n",
    "except Exception as e:\n",
    "    panel_res = None\n",
    "    print(\"PanelOLS not available or failed:\", e)\n",
    "    # Fallback: OLS with entity dummies (country dummies)\n",
    "    try:\n",
    "        # create country dummies (but beware many dummies)\n",
    "        country_dummies = pd.get_dummies(mod_df_reg.index.get_level_values('ISO3') if hasattr(mod_df_reg.index, 'get_level_values') else mod_df_reg['ISO3'], prefix='c', drop_first=True)\n",
    "    except Exception:\n",
    "        country_dummies = pd.get_dummies(mod_df_reg['ISO3'], prefix='c', drop_first=True)\n",
    "    try:\n",
    "        mod_with_c = pd.concat([mod_df_reg.reset_index(drop=True), country_dummies.reset_index(drop=True)], axis=1)\n",
    "        preds2 = [p for p in predictors if p in mod_with_c.columns] + list(country_dummies.columns)\n",
    "        formula2 = \"Suicide_rate ~ \" + \" + \".join(preds2)\n",
    "        res_with_country = smf.ols(formula=formula2, data=mod_with_c).fit(cov_type='HC1')\n",
    "        with open(OUTDIR / \"model_with_country_dummies_summary.txt\", \"w\", encoding=\"utf-8\") as f:\n",
    "            f.write(res_with_country.summary().as_text())\n",
    "        print(\"Saved OLS with country dummies summary (fallback).\")\n",
    "    except Exception as e2:\n",
    "        print(\"Fallback country-dummy model failed:\", e2)\n",
    "\n",
    "# ---------- DIAGNOSTICS ----------\n",
    "# VIF on predictors (excluding dummies if many)\n",
    "from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
    "vif_covars = [p for p in predictors if p in mod_df_reg.columns and not p.startswith('Y_') and not p.startswith('cont_')]\n",
    "if vif_covars:\n",
    "    X_vif = sm.add_constant(mod_df_reg[vif_covars].fillna(0))\n",
    "    vif_df = pd.DataFrame({\n",
    "        'variable': X_vif.columns,\n",
    "        'VIF': [variance_inflation_factor(X_vif.values, i) for i in range(X_vif.shape[1])]\n",
    "    })\n",
    "    vif_df.to_csv(OUTDIR / \"vif.csv\", index=False)\n",
    "\n",
    "# Heteroskedasticity: Breusch-Pagan\n",
    "if pooled_res is not None:\n",
    "    try:\n",
    "        bp = sm.stats.diagnostic.het_breuschpagan(pooled_res.resid, pooled_res.model.exog)\n",
    "        with open(OUTDIR / \"breusch_pagan.txt\", \"w\", encoding=\"utf-8\") as f:\n",
    "            f.write(f\"LM stat: {bp[0]}, p-value: {bp[1]}, fvalue: {bp[2]}, f p-value: {bp[3]}\")\n",
    "    except Exception as e:\n",
    "        print(\"BP test failed:\", e)\n",
    "\n",
    "# ---------- SAVE final dataset ----------\n",
    "# Keep a well-documented final CSV with imputation flags and new features\n",
    "save_cols = df_out.columns.tolist()\n",
    "df_out.to_csv(FINAL_CSV, index=False)\n",
    "print(\"Saved final dataset to:\", FINAL_CSV)\n",
    "\n",
    "# Brief final printout\n",
    "print(\"Pipeline complete. Outputs in:\", OUTDIR)\n",
    "print(\"Final dataset shape:\", df_out.shape)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
