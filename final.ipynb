{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "066a0a14",
   "metadata": {},
   "source": [
    "# ASA: International Data Quest: The Price of Progress"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "959c5031",
   "metadata": {},
   "source": [
    "## Executive Summary"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bced64f",
   "metadata": {},
   "source": [
    "# Data Management"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "57d12e0c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "✅ Data akhir siap dipakai secara etis & transparan:\n",
      "                  Country Name  GDP_per_capita  Suicide_rate HDI_2023  \\\n",
      "0                  Afghanistan      485.022544      3.545000    0.496   \n",
      "1  Africa Eastern and Southern     1429.887639      8.082190      NaN   \n",
      "2   Africa Western and Central     1806.725660      5.590313      NaN   \n",
      "3                      Albania     4702.593151      3.430000     0.81   \n",
      "4                      Algeria     4550.913354      2.002500    0.763   \n",
      "\n",
      "   GDP_per_capita_coverage  Suicide_rate_coverage Low_data_quality_flag  \n",
      "0                      1.0                    0.8     ✅ Sufficient data  \n",
      "1                      1.0                    0.8     ✅ Sufficient data  \n",
      "2                      1.0                    0.8     ✅ Sufficient data  \n",
      "3                      1.0                    0.8     ✅ Sufficient data  \n",
      "4                      1.0                    0.8     ✅ Sufficient data  \n",
      "\n",
      "📊 Korelasi Matrix:\n",
      "                HDI_2023  GDP_per_capita  Suicide_rate\n",
      "HDI_2023           1.000           0.690         0.228\n",
      "GDP_per_capita     0.690           1.000         0.186\n",
      "Suicide_rate       0.228           0.186         1.000\n",
      "\n",
      "HDI ↔ Suicide Rate: r = 0.228, p = 0.0014\n",
      "GDP ↔ Suicide Rate: r = 0.186, p = 0.0095\n",
      "\n",
      "Jumlah negara valid dalam analisis korelasi: 193\n",
      "\n",
      "📘 Catatan Etis:\n",
      "Kami tidak menghapus seluruh observasi dengan data hilang.\n",
      "Sebaliknya, kami melakukan interpolasi dan memberi penanda 'Low data reliability'\n",
      "untuk negara dengan cakupan data <60%, agar analisis tetap inklusif sesuai prinsip ASA 2.2.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ASUS\\AppData\\Local\\Temp\\ipykernel_25188\\726895398.py:92: FutureWarning: DataFrame.interpolate with object dtype is deprecated and will raise in a future version. Call obj.infer_objects(copy=False) before interpolating instead.\n",
      "  ].interpolate(method='linear', limit_direction='both')\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy.stats import pearsonr\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# ================================\n",
    "# 1️⃣ LOAD & BERSIHKAN HDI\n",
    "# ================================\n",
    "df_hdi = pd.read_excel(\"./External-Dataset/HDI.xlsx\")\n",
    "# df_hdi = pd.read_excel(\"HDI.xlsx\")\n",
    "\n",
    "df_hdi.columns = df_hdi.columns.str.strip()\n",
    "df_hdi = df_hdi[['Country', 'Human Development Index (HDI)']]\n",
    "df_hdi = df_hdi.rename(columns={\n",
    "    'Country': 'Country Name',\n",
    "    'Human Development Index (HDI)': 'HDI_2023'\n",
    "})\n",
    "df_hdi['Country Name'] = df_hdi['Country Name'].str.strip()\n",
    "\n",
    "# ================================\n",
    "# 2️⃣ LOAD & SAMAKAN NAMA NEGARA\n",
    "# ================================\n",
    "df = pd.read_csv(\"./Dataset/WDICSV-rev.csv\")\n",
    "# df = pd.read_csv(\"WDICSV-rev.csv\")\n",
    "\n",
    "rename_map = {\n",
    "    'Bahamas': 'Bahamas, The',\n",
    "    'Bolivia (Plurinational State of)': 'Bolivia',\n",
    "    'Congo': 'Congo, Rep.',\n",
    "    'Congo (Democratic Republic of the)': 'Congo, Dem. Rep.',\n",
    "    \"Côte d'Ivoire\": \"Cote d'Ivoire\",\n",
    "    'Egypt': 'Egypt, Arab Rep.',\n",
    "    'Eswatini (Kingdom of)': 'Eswatini',\n",
    "    'Gambia': 'Gambia, The',\n",
    "    'Hong Kong, China (SAR)': 'Hong Kong SAR, China',\n",
    "    'Iran (Islamic Republic of)': 'Iran, Islamic Rep.',\n",
    "    \"Korea (Democratic People's Rep. of)\": 'Korea, Dem. People’s Rep.',\n",
    "    'Korea (Republic of)': 'Korea, Rep.',\n",
    "    'Kyrgyzstan': 'Kyrgyz Republic',\n",
    "    \"Lao People's Democratic Republic\": 'Lao PDR',\n",
    "    'Micronesia (Federated States of)': 'Micronesia, Fed. Sts.',\n",
    "    'Moldova (Republic of)': 'Moldova',\n",
    "    'Palestine, State of': 'West Bank and Gaza',\n",
    "    'Saint Kitts and Nevis': 'St. Kitts and Nevis',\n",
    "    'Saint Lucia': 'St. Lucia',\n",
    "    'Saint Vincent and the Grenadines': 'St. Vincent and the Grenadines',\n",
    "    'Korea, Dem. People’s Rep.': \"Korea, Dem. People's Rep.\",\n",
    "    'Slovakia': 'Slovak Republic',\n",
    "    'Tanzania (United Republic of)': 'Tanzania',\n",
    "    'Türkiye': 'Turkiye',\n",
    "    'Venezuela (Bolivarian Republic of)': 'Venezuela, RB',\n",
    "    'Yemen': 'Yemen, Rep.'\n",
    "}\n",
    "df_hdi['Country Name'] = df_hdi['Country Name'].replace(rename_map)\n",
    "\n",
    "# ================================\n",
    "# 3️⃣ PILIH INDIKATOR & HITUNG RATA-RATA 5 TAHUN\n",
    "# ================================\n",
    "indicators = {\n",
    "    \"NY.GDP.PCAP.KD\": \"GDP_per_capita\",\n",
    "    \"SH.STA.SUIC.P5\": \"Suicide_rate\"\n",
    "}\n",
    "years = ['2018', '2019', '2020', '2021', '2022']\n",
    "\n",
    "df_sel = df[df[\"Indicator Code\"].isin(indicators.keys())].copy()\n",
    "\n",
    "# Hitung mean dan coverage (berapa persen tahun yang punya data)\n",
    "df_sel[\"mean_5yr\"] = df_sel[years].mean(axis=1, skipna=True)\n",
    "df_sel[\"data_coverage\"] = df_sel[years].notna().sum(axis=1) / len(years)\n",
    "\n",
    "df_mean = df_sel[[\"Country Name\", \"Indicator Code\", \"mean_5yr\", \"data_coverage\"]].drop_duplicates()\n",
    "\n",
    "df_pivot = df_mean.pivot(index=\"Country Name\", columns=\"Indicator Code\", values=\"mean_5yr\").reset_index()\n",
    "df_pivot = df_pivot.rename(columns=indicators)\n",
    "\n",
    "df_coverage = df_mean.pivot(index=\"Country Name\", columns=\"Indicator Code\", values=\"data_coverage\").reset_index()\n",
    "df_coverage = df_coverage.rename(columns={code: indicators[code] + \"_coverage\" for code in indicators})\n",
    "\n",
    "# ================================\n",
    "# 4️⃣ GABUNGKAN SEMUA DATA\n",
    "# ================================\n",
    "df_merge_mean = (\n",
    "    df_pivot\n",
    "    .merge(df_hdi, on=\"Country Name\", how=\"left\")\n",
    "    .merge(df_coverage, on=\"Country Name\", how=\"left\")\n",
    ")\n",
    "\n",
    "# Interpolasi nilai yang hilang untuk inklusivitas\n",
    "df_merge_mean[['GDP_per_capita', 'Suicide_rate', 'HDI_2023']] = df_merge_mean[\n",
    "    ['GDP_per_capita', 'Suicide_rate', 'HDI_2023']\n",
    "].interpolate(method='linear', limit_direction='both')\n",
    "\n",
    "# Tambahkan flag etis: data reliability\n",
    "df_merge_mean[\"Low_data_quality_flag\"] = np.where(\n",
    "    (df_merge_mean[\"GDP_per_capita_coverage\"] < 0.6) |\n",
    "    (df_merge_mean[\"Suicide_rate_coverage\"] < 0.6),\n",
    "    \"⚠️ Low data reliability\",\n",
    "    \"✅ Sufficient data\"\n",
    ")\n",
    "\n",
    "print(\"\\n✅ Data akhir siap dipakai secara etis & transparan:\")\n",
    "print(df_merge_mean.head())\n",
    "\n",
    "# ================================\n",
    "# 5️⃣ ANALISIS KORELASI\n",
    "# ================================\n",
    "for col in ['HDI_2023', 'GDP_per_capita', 'Suicide_rate']:\n",
    "    df_merge_mean[col] = pd.to_numeric(df_merge_mean[col], errors='coerce')\n",
    "\n",
    "# Hapus baris yang punya NaN atau inf di kolom penting\n",
    "df_corr = df_merge_mean.replace([np.inf, -np.inf], np.nan).dropna(\n",
    "    subset=['HDI_2023', 'GDP_per_capita', 'Suicide_rate']\n",
    ")\n",
    "\n",
    "# Sekarang aman untuk pearson\n",
    "corr_matrix = df_corr[['HDI_2023', 'GDP_per_capita', 'Suicide_rate']].corr(method='pearson')\n",
    "\n",
    "r_hdi_suicide, p_hdi_suicide = pearsonr(df_corr['HDI_2023'], df_corr['Suicide_rate'])\n",
    "r_gdp_suicide, p_gdp_suicide = pearsonr(df_corr['GDP_per_capita'], df_corr['Suicide_rate'])\n",
    "\n",
    "print(\"\\n📊 Korelasi Matrix:\")\n",
    "print(corr_matrix.round(3))\n",
    "print(f\"\\nHDI ↔ Suicide Rate: r = {r_hdi_suicide:.3f}, p = {p_hdi_suicide:.4f}\")\n",
    "print(f\"GDP ↔ Suicide Rate: r = {r_gdp_suicide:.3f}, p = {p_gdp_suicide:.4f}\")\n",
    "\n",
    "# Catatan kecil tambahan (opsional)\n",
    "print(f\"\\nJumlah negara valid dalam analisis korelasi: {len(df_corr)}\")\n",
    "\n",
    "# ================================\n",
    "# 6️⃣ CATATAN ETIS\n",
    "# ================================\n",
    "print(\"\\n📘 Catatan Etis:\")\n",
    "print(\"Kami tidak menghapus seluruh observasi dengan data hilang.\")\n",
    "print(\"Sebaliknya, kami melakukan interpolasi dan memberi penanda 'Low data reliability'\")\n",
    "print(\"untuk negara dengan cakupan data <60%, agar analisis tetap inklusif sesuai prinsip ASA 2.2.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bbf1d694",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "nan not found in ISO3\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded base cleaned cross-section: Output\\merged_clean_panel.csv\n",
      "Loaded HDI timeseries from Time-Series-Dataset\\hdi-time-series-data.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "nan not found in ISO3\n",
      "nan not found in ISO3\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved merged panel: new_analysis_output\\merged_panel_time_series.csv\n",
      "Saved EDA summary to new_analysis_output\\eda_panel_summary.txt\n",
      "Saved time series plot: new_analysis_output\\time_series_top_suicide.png\n",
      "Saved scatter plot: new_analysis_output\\scatter_hdi_panel.png\n",
      "Saved choropleth: new_analysis_output\\choropleth_2023.html\n",
      "Model formula: Suicide_rate ~ HDI + HDI_sq + log_GDP_per_capita + cont_America + cont_Asia + cont_Europe + cont_Oceania + Y_2020 + Y_2021\n",
      "Saved model summary to new_analysis_output\\panel_model_summary.txt\n",
      "Pipeline complete. Files in new_analysis_output\n",
      "\n",
      "Sample of merged panel (first 10 rows):\n",
      "Country Name ISO3  Year   HDI  Suicide_rate  GDP_per_capita  log_GDP_per_capita income_group_auto continent Low_data_quality_flag   HDI_sq  Suicide_rate_lag1  HDI_lag1 Low_data_quality_flag_panel\n",
      " Afghanistan  AFG  2010 0.449           NaN      485.022544            6.184195               Low      Asia     ✅ Sufficient data 0.201601                NaN       NaN     ⚠️ Low data reliability\n",
      " Afghanistan  AFG  2019 0.492           NaN      485.022544            6.184195               Low      Asia     ✅ Sufficient data 0.242064                NaN     0.449     ⚠️ Low data reliability\n",
      " Afghanistan  AFG  2020 0.488          3.63      485.022544            6.184195               Low      Asia     ✅ Sufficient data 0.238144                NaN     0.492           ✅ Sufficient data\n",
      " Afghanistan  AFG  2021 0.473          3.60      485.022544            6.184195               Low      Asia     ✅ Sufficient data 0.223729               3.63     0.488           ✅ Sufficient data\n",
      " Afghanistan  AFG  2022 0.462           NaN      485.022544            6.184195               Low      Asia     ✅ Sufficient data 0.213444               3.60     0.473     ⚠️ Low data reliability\n",
      " Afghanistan  AFG  2023 0.496           NaN      485.022544            6.184195               Low      Asia     ✅ Sufficient data 0.246016                NaN     0.462     ⚠️ Low data reliability\n",
      "      Angola  AGO  2010 0.516           NaN     2528.206553            7.835265      Lower-Middle    Africa     ✅ Sufficient data 0.266256                NaN       NaN     ⚠️ Low data reliability\n",
      "      Angola  AGO  2019 0.597           NaN     2528.206553            7.835265      Lower-Middle    Africa     ✅ Sufficient data 0.356409                NaN     0.516     ⚠️ Low data reliability\n",
      "      Angola  AGO  2020 0.594          6.90     2528.206553            7.835265      Lower-Middle    Africa     ✅ Sufficient data 0.352836                NaN     0.597           ✅ Sufficient data\n",
      "      Angola  AGO  2021 0.590          6.95     2528.206553            7.835265      Lower-Middle    Africa     ✅ Sufficient data 0.348100               6.90     0.594           ✅ Sufficient data\n"
     ]
    }
   ],
   "source": [
    "# full_panel_pipeline.py\n",
    "# Run this in one go in Jupyter / VSCode / Colab.\n",
    "# Produces merged_panel_time_series.csv, EDA outputs, plots, model summary in ./new_analysis_output/\n",
    "\n",
    "import os\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import statsmodels.formula.api as smf\n",
    "import statsmodels.api as sm\n",
    "from io import StringIO\n",
    "\n",
    "sns.set(style=\"whitegrid\")\n",
    "\n",
    "# ---------- CONFIG ----------\n",
    "BASE_PATH = Path(\"./Output/merged_clean_panel.csv\")   # path to your existing cleaned cross-sectional file\n",
    "HDI_PATH = Path(\"./Time-Series-Dataset/hdi-time-series-data.csv\")         # optional: full file\n",
    "SU19_PATH = Path(\"./Time-Series-Dataset/suicide-rate-2019.csv\")\n",
    "SU20_PATH = Path(\"./Time-Series-Dataset/suicide-rate-by-country-2020.csv\")\n",
    "SU21_PATH = Path(\"./Time-Series-Dataset/suicide-rate-by-country-2021.csv\")\n",
    "\n",
    "OUT = Path(\"./new_analysis_output\")\n",
    "OUT.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# ---------- helper: ISO2 -> ISO3 ----------\n",
    "def try_import_country_converter():\n",
    "    try:\n",
    "        import country_converter as coco\n",
    "        return coco.CountryConverter()\n",
    "    except Exception:\n",
    "        try:\n",
    "            import pycountry\n",
    "            return pycountry\n",
    "        except Exception:\n",
    "            return None\n",
    "\n",
    "cc = try_import_country_converter()\n",
    "\n",
    "def iso2_to_iso3(code):\n",
    "    if pd.isna(code):\n",
    "        return np.nan\n",
    "    code = str(code).strip()\n",
    "    if not code:\n",
    "        return np.nan\n",
    "    # try country_converter\n",
    "    if cc is None:\n",
    "        return np.nan\n",
    "    try:\n",
    "        # cc may be CountryConverter or pycountry module\n",
    "        if hasattr(cc, \"convert\"):\n",
    "            return cc.convert(names=code, to='ISO3')\n",
    "        else:\n",
    "            # pycountry fallback: iso2 -> alpha_3\n",
    "            c = cc.countries.get(alpha_2=code.upper())\n",
    "            return c.alpha_3 if c else np.nan\n",
    "    except Exception:\n",
    "        return np.nan\n",
    "\n",
    "# ---------- 1) load base cleaned dataset ----------\n",
    "if BASE_PATH.exists():\n",
    "    df_base = pd.read_csv(BASE_PATH)\n",
    "    print(\"Loaded base cleaned cross-section:\", BASE_PATH)\n",
    "else:\n",
    "    # try alt paths or raise helpful message\n",
    "    alt = Path(\"/mnt/data/merged_clean_panel.csv\")\n",
    "    if alt.exists():\n",
    "        df_base = pd.read_csv(alt)\n",
    "        print(\"Loaded base cleaned cross-section:\", alt)\n",
    "    else:\n",
    "        raise FileNotFoundError(f\"Base cleaned file not found at {BASE_PATH} or /mnt/data. Please provide merged_clean_panel.csv.\")\n",
    "\n",
    "# ---------- 2) load HDI time series (wide) ----------\n",
    "if HDI_PATH.exists():\n",
    "    df_hdi_ts = pd.read_csv(HDI_PATH)\n",
    "    print(\"Loaded HDI timeseries from\", HDI_PATH)\n",
    "else:\n",
    "    # fallback sample (use your full file for real analysis)\n",
    "    sample = '''\"flagCode\",\"country\",\"HumanDevelopmentIndex_2023\",\"HumanDevelopmentIndex_2022\",\"HumanDevelopmentIndex_2021\",\"HumanDevelopmentIndex_2020\",\"HumanDevelopmentIndex_2019\",\"HumanDevelopmentIndex_2010\",\"HumanDevelopmentIndex_HDITierCurrent_txt_YearFree\"\n",
    "\"IS\",\"Iceland\",0.972,0.959,0.957,0.955,0.958,0.927,\"Very High\"\n",
    "\"CH\",\"Switzerland\",0.97,0.967,0.965,0.957,0.96,0.94,\"Very High\"\n",
    "\"NO\",\"Norway\",0.97,0.966,0.964,0.963,0.961,0.938,\"Very High\"\n",
    "\"DK\",\"Denmark\",0.962,0.952,0.947,0.946,0.946,0.913,\"Very High\"\n",
    "\"DE\",\"Germany\",0.959,0.95,0.948,0.948,0.951,0.929,\"Very High\"\n",
    "\"SE\",\"Sweden\",0.959,0.952,0.949,0.944,0.947,0.91,\"Very High\"'''\n",
    "    df_hdi_ts = pd.read_csv(StringIO(sample))\n",
    "    print(\"HDI timeseries file not found. Using sample rows (replace with full file).\")\n",
    "\n",
    "# add ISO3 column from flagCode if exists\n",
    "if 'flagCode' in df_hdi_ts.columns:\n",
    "    df_hdi_ts['ISO2'] = df_hdi_ts['flagCode'].astype(str).str.replace('\"','').str.strip()\n",
    "    df_hdi_ts['ISO3'] = df_hdi_ts['ISO2'].apply(iso2_to_iso3)\n",
    "else:\n",
    "    # attempt to convert country names to ISO3\n",
    "    if 'country' in df_hdi_ts.columns:\n",
    "        df_hdi_ts['ISO3'] = df_hdi_ts['country'].apply(iso2_to_iso3)\n",
    "\n",
    "# ---------- 3) load suicide yearly files and stack ----------\n",
    "def load_or_sample(path, sample_text):\n",
    "    if Path(path).exists():\n",
    "        return pd.read_csv(path)\n",
    "    else:\n",
    "        return pd.read_csv(StringIO(sample_text))\n",
    "\n",
    "su19_sample = '''\"flagCode\",\"country\",\"SuicideRateCountries_2019\",\"SuicideRateMaleCountries_2019\",\"SuicideRateFemaleCountries_2019\"\n",
    "\"GY\",\"Guyana\",40.3,63,17.4\n",
    "\"LT\",\"Lithuania\",26.1,45.4,9.6\n",
    "\"KR\",\"South Korea\",28.6,40.2,16.9\n",
    "\"RU\",\"Russia\",25.1,43.6,9.1\n",
    "\"SR\",\"Suriname\",25.4,38.8,11.8\n",
    "'''\n",
    "su20_sample = '''\"flagCode\",\"country\",\"SuicideRateCountries_2020\",\"SuicideRateMaleCountries_2020\",\"SuicideRateFemaleCountries_2020\"\n",
    "\"GL\",\"Greenland\",64.6,94.39,31.59\n",
    "\"GY\",\"Guyana\",31.71,53.32,10.59\n",
    "\"LT\",\"Lithuania\",28.07,50.46,8.93\n",
    "\"KR\",\"South Korea\",25.63,36.61,14.56\n",
    "\"RU\",\"Russia\",24.25,42.42,8.44\n",
    "'''\n",
    "su21_sample = '''\"flagCode\",\"country\",\"SuicideRateCountries_2021\",\"SuicideRateMaleCountries_2021\",\"SuicideRateFemaleCountries_2021\"\n",
    "\"GL\",\"Greenland\",59.62,86.96,29.4\n",
    "\"GY\",\"Guyana\",31.26,52.79,10.31\n",
    "\"LT\",\"Lithuania\",27.91,49.96,9.07\n",
    "\"KR\",\"South Korea\",25.81,36.96,14.58\n",
    "\"RU\",\"Russia\",24.1,41.82,8.66\n",
    "'''\n",
    "df_su19 = load_or_sample(SU19_PATH, su19_sample)\n",
    "df_su20 = load_or_sample(SU20_PATH, su20_sample)\n",
    "df_su21 = load_or_sample(SU21_PATH, su21_sample)\n",
    "\n",
    "# add ISO3\n",
    "for d in (df_su19, df_su20, df_su21):\n",
    "    if 'flagCode' in d.columns:\n",
    "        d['ISO2'] = d['flagCode'].astype(str).str.replace('\"','').str.strip()\n",
    "        d['ISO3'] = d['ISO2'].apply(iso2_to_iso3)\n",
    "    else:\n",
    "        d['ISO3'] = d['country'].apply(iso2_to_iso3)\n",
    "\n",
    "# helper to create long format per year\n",
    "def su_to_long(df_su, year):\n",
    "    # find main suicide column (choose first numeric column not ISO/flag)\n",
    "    numeric_cols = [c for c in df_su.columns if df_su[c].dtype.kind in 'fi']\n",
    "    # prefer column containing year\n",
    "    col = None\n",
    "    for c in numeric_cols:\n",
    "        if str(year) in c:\n",
    "            col = c\n",
    "            break\n",
    "    if col is None and numeric_cols:\n",
    "        col = numeric_cols[0]\n",
    "    long = df_su[['country','ISO3', col]].rename(columns={col:'Suicide_rate'}).copy()\n",
    "    long['Year'] = int(year)\n",
    "    return long.dropna(subset=['ISO3','Suicide_rate'])\n",
    "\n",
    "su19_long = su_to_long(df_su19, 2019)\n",
    "su20_long = su_to_long(df_su20, 2020)\n",
    "su21_long = su_to_long(df_su21, 2021)\n",
    "df_su_long = pd.concat([su19_long, su20_long, su21_long], ignore_index=True)\n",
    "\n",
    "# ---------- 4) melt HDI ts wide->long ----------\n",
    "hdi_cols = [c for c in df_hdi_ts.columns if c.startswith('HumanDevelopmentIndex_') and c.split('_')[-1].isdigit()]\n",
    "if not hdi_cols:\n",
    "    # try other heuristics\n",
    "    hdi_cols = [c for c in df_hdi_ts.columns if 'HumanDevelopment' in c and any(ch.isdigit() for ch in c)]\n",
    "hdi_long = df_hdi_ts.melt(id_vars=[col for col in ['flagCode','country','ISO3'] if col in df_hdi_ts.columns],\n",
    "                           value_vars=hdi_cols, var_name='hdi_var', value_name='HDI')\n",
    "# parse year\n",
    "hdi_long['Year'] = hdi_long['hdi_var'].str.replace('HumanDevelopmentIndex_','').astype(int)\n",
    "hdi_long = hdi_long[['country','ISO3','Year','HDI']].dropna(subset=['ISO3','HDI'])\n",
    "\n",
    "# ---------- 5) Merge HDI long & suicide long -> panel\n",
    "panel = pd.merge(hdi_long, df_su_long, on=['ISO3','Year'], how='outer', suffixes=('_hdi','_su'))\n",
    "# bring in base cross-sectional features by ISO3\n",
    "if 'ISO3' in df_base.columns:\n",
    "    base_for_merge = df_base.copy()\n",
    "    # ensure ISO3 exists in base\n",
    "    if 'ISO3' not in base_for_merge.columns:\n",
    "        # try converting Country Name to ISO3\n",
    "        base_for_merge['ISO3'] = base_for_merge['Country Name'].apply(iso2_to_iso3)\n",
    "    panel = panel.merge(base_for_merge, on='ISO3', how='left', suffixes=('','_base'))\n",
    "else:\n",
    "    panel = panel.merge(df_base, left_on='country_hdi', right_on='Country Name', how='left')\n",
    "\n",
    "# derive Country Name\n",
    "panel['Country Name'] = panel['country_hdi'].fillna(panel['country_su']).fillna(panel.get('Country Name'))\n",
    "\n",
    "# reorder\n",
    "cols_keep = ['Country Name','ISO3','Year','HDI','Suicide_rate','GDP_per_capita','log_GDP_per_capita','income_group_auto','continent','Low_data_quality_flag']\n",
    "panel = panel[[c for c in cols_keep if c in panel.columns]]\n",
    "\n",
    "# ---------- 6) Feature engineering & cleaning ----------\n",
    "# numeric coercion\n",
    "for c in ['HDI','Suicide_rate','GDP_per_capita','log_GDP_per_capita']:\n",
    "    if c in panel.columns:\n",
    "        panel[c] = pd.to_numeric(panel[c], errors='coerce')\n",
    "\n",
    "panel = panel.sort_values(['ISO3','Year'])\n",
    "panel['HDI_sq'] = panel['HDI']**2\n",
    "panel['Suicide_rate_lag1'] = panel.groupby('ISO3')['Suicide_rate'].shift(1)\n",
    "panel['HDI_lag1'] = panel.groupby('ISO3')['HDI'].shift(1)\n",
    "panel['Low_data_quality_flag_panel'] = np.where(panel['GDP_per_capita'].isna() | panel['Suicide_rate'].isna(), \"⚠️ Low data reliability\", \"✅ Sufficient data\")\n",
    "\n",
    "# save cleaned panel\n",
    "out_csv = OUT / \"merged_panel_time_series.csv\"\n",
    "panel.to_csv(out_csv, index=False)\n",
    "print(\"Saved merged panel:\", out_csv)\n",
    "\n",
    "# ---------- 7) EDA summary ----------\n",
    "eda_txt = OUT / \"eda_panel_summary.txt\"\n",
    "with open(eda_txt, \"w\", encoding=\"utf-8\") as f:\n",
    "    f.write(\"EDA Summary - merged panel\\n\")\n",
    "    f.write(f\"Rows: {len(panel)}, unique countries: {panel['ISO3'].nunique()}\\n\\n\")\n",
    "    f.write(\"Descriptive stats (HDI, Suicide_rate, GDP_per_capita):\\n\")\n",
    "    if 'HDI' in panel.columns:\n",
    "        f.write(panel[['HDI','Suicide_rate','GDP_per_capita']].describe().to_string())\n",
    "    else:\n",
    "        f.write(\"HDI missing\\n\")\n",
    "print(\"Saved EDA summary to\", eda_txt)\n",
    "\n",
    "# ---------- 8) Visualizations ----------\n",
    "# (A) Time series: sample top average suicide countries\n",
    "plt.figure(figsize=(10,5))\n",
    "if 'Suicide_rate' in panel.columns:\n",
    "    top_iso = panel.groupby('ISO3')['Suicide_rate'].mean().dropna().sort_values(ascending=False).head(6).index.tolist()\n",
    "    for iso in top_iso:\n",
    "        sub = panel[panel['ISO3']==iso]\n",
    "        plt.plot(sub['Year'], sub['Suicide_rate'], marker='o', label=iso)\n",
    "    plt.legend()\n",
    "    plt.title(\"Time series suicide rate (sample top avg countries)\")\n",
    "    plt.xlabel(\"Year\"); plt.ylabel(\"Suicide rate\")\n",
    "    plt.tight_layout()\n",
    "    ts_path = OUT / \"time_series_top_suicide.png\"\n",
    "    plt.savefig(ts_path); plt.close()\n",
    "    print(\"Saved time series plot:\", ts_path)\n",
    "else:\n",
    "    print(\"Suicide_rate not present for plotting time series.\")\n",
    "\n",
    "# (B) Scatter HDI vs Suicide (panel combined)\n",
    "if 'HDI' in panel.columns and 'Suicide_rate' in panel.columns:\n",
    "    plt.figure(figsize=(8,6))\n",
    "    sns.scatterplot(data=panel, x='HDI', y='Suicide_rate', hue='Year', palette='viridis', alpha=0.8)\n",
    "    plt.title(\"HDI vs Suicide Rate (panel years combined)\")\n",
    "    scatter_path = OUT / \"scatter_hdi_panel.png\"\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(scatter_path); plt.close()\n",
    "    print(\"Saved scatter plot:\", scatter_path)\n",
    "\n",
    "# (C) Choropleth latest year (if plotly available)\n",
    "latest_year = int(panel['Year'].dropna().max()) if 'Year' in panel.columns else None\n",
    "if latest_year:\n",
    "    latest = panel[panel['Year']==latest_year]\n",
    "    try:\n",
    "        import plotly.express as px\n",
    "        fig = px.choropleth(latest, locations='ISO3', color='Suicide_rate',\n",
    "                            hover_name='Country Name', color_continuous_scale='Reds',\n",
    "                            title=f'Suicide rate ({latest_year})')\n",
    "        map_html = OUT / f'choropleth_{latest_year}.html'\n",
    "        fig.write_html(map_html)\n",
    "        print(\"Saved choropleth:\", map_html)\n",
    "    except Exception as e:\n",
    "        print(\"Plotly not available or choropleth failed:\", e)\n",
    "\n",
    "# ---------- 9) Modeling: pooled OLS with year & continent dummies ----------\n",
    "mod_df = panel.dropna(subset=['Suicide_rate','HDI','HDI_sq']).copy()\n",
    "# add log GDP if missing and GDP present\n",
    "if 'log_GDP_per_capita' not in mod_df.columns and 'GDP_per_capita' in mod_df.columns:\n",
    "    mod_df['log_GDP_per_capita'] = np.log(mod_df['GDP_per_capita'].where(mod_df['GDP_per_capita']>0, np.nan))\n",
    "\n",
    "# create continent dummies if present\n",
    "if 'continent' in mod_df.columns:\n",
    "    cont_dummies = pd.get_dummies(mod_df['continent'], prefix='cont', drop_first=True)\n",
    "    mod_df = pd.concat([mod_df, cont_dummies], axis=1)\n",
    "else:\n",
    "    cont_dummies = pd.DataFrame()\n",
    "\n",
    "# year dummies\n",
    "if 'Year' in mod_df.columns:\n",
    "    year_dummies = pd.get_dummies(mod_df['Year'].astype(int).astype(str), prefix='Y', drop_first=True)\n",
    "    mod_df = pd.concat([mod_df, year_dummies], axis=1)\n",
    "else:\n",
    "    year_dummies = pd.DataFrame()\n",
    "\n",
    "# build formula: Suicide_rate ~ HDI + HDI_sq + log_GDP + continent dummies + year dummies\n",
    "preds = ['HDI','HDI_sq']\n",
    "if 'log_GDP_per_capita' in mod_df.columns:\n",
    "    preds.append('log_GDP_per_capita')\n",
    "preds += list(cont_dummies.columns) + list(year_dummies.columns)\n",
    "formula = \"Suicide_rate ~ \" + \" + \".join(preds)\n",
    "print(\"Model formula:\", formula)\n",
    "\n",
    "try:\n",
    "    res = smf.ols(formula=formula, data=mod_df).fit(cov_type='HC1')\n",
    "    model_summary_path = OUT / \"panel_model_summary.txt\"\n",
    "    with open(model_summary_path, \"w\", encoding=\"utf-8\") as f:\n",
    "        f.write(res.summary().as_text())\n",
    "    print(\"Saved model summary to\", model_summary_path)\n",
    "except Exception as e:\n",
    "    print(\"Model fitting failed:\", e)\n",
    "    res = None\n",
    "\n",
    "# ---------- 10) Final save list ----------\n",
    "files = list(OUT.iterdir())\n",
    "with open(OUT / \"output_files_list.txt\", \"w\", encoding=\"utf-8\") as f:\n",
    "    for p in files:\n",
    "        f.write(str(p) + \"\\n\")\n",
    "\n",
    "print(\"Pipeline complete. Files in\", OUT)\n",
    "print(\"\\nSample of merged panel (first 10 rows):\")\n",
    "print(panel.head(10).to_string(index=False))\n",
    "\n",
    "# End of script\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d22432ed",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
